{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wan 2.2 Video Generation with SGLang (udocker)\n",
    "\n",
    "Run Wan 2.2 text-to-video model in Google Colab using udocker.\n",
    "\n",
    "**Requirements:**\n",
    "- Colab with GPU (T4, A100, or L4)\n",
    "- ~40GB disk space for model\n",
    "- ~20GB for Docker image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install udocker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install udocker\n",
    "!pip install udocker\n",
    "!udocker install"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Docker Image\n",
    "\n",
    "Option A: Load from tar file (if you uploaded it to Google Drive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive (if using tar from Drive)\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load image from tar file\n",
    "!udocker load -i /content/drive/MyDrive/glm-image-sglang-v0.5.0.tar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Option B: Pull from Docker Hub (Recommended)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Pull from Docker Hub (recommended - no need to upload tar file)\n!udocker pull khalidnass/glm-image-sglang:v0.5.0"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create Container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create container from image\n!udocker create --name=wan22 khalidnass/glm-image-sglang:v0.5.0"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Download Wan 2.2 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install huggingface_hub for downloading\n",
    "!pip install -q huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Wan 2.2 T2V model (14B parameters, ~40GB)\n",
    "# This takes 10-30 minutes depending on connection\n",
    "from huggingface_hub import snapshot_download\n",
    "import os\n",
    "\n",
    "os.makedirs('/content/models', exist_ok=True)\n",
    "\n",
    "snapshot_download(\n",
    "    repo_id='Wan-AI/Wan2.2-T2V-A14B-Diffusers',\n",
    "    local_dir='/content/models/Wan2.2-T2V-A14B-Diffusers',\n",
    "    local_dir_use_symlinks=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative: Download smaller TI2V model (5B, ~15GB)\n",
    "# snapshot_download(\n",
    "#     repo_id='Wan-AI/Wan2.2-TI2V-5B-Diffusers',\n",
    "#     local_dir='/content/models/Wan2.2-TI2V-5B-Diffusers',\n",
    "#     local_dir_use_symlinks=False\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Run SGLang Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set execution mode for GPU support\n",
    "!udocker setup --nvidia wan22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash --bg\n",
    "# Run SGLang server in background\n",
    "udocker run \\\n",
    "  --volume=/content/models:/app/models \\\n",
    "  --env=\"MODEL_PATH=/app/models/Wan2.2-T2V-A14B-Diffusers\" \\\n",
    "  --env=\"HF_HUB_OFFLINE=1\" \\\n",
    "  wan22 \\\n",
    "  sglang serve --model-path /app/models/Wan2.2-T2V-A14B-Diffusers --port 30000 --host 0.0.0.0 \\\n",
    "  > /content/sglang.log 2>&1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait for server to start (check logs)\n",
    "import time\n",
    "print(\"Waiting for server to start (2-5 minutes)...\")\n",
    "time.sleep(120)\n",
    "!tail -50 /content/sglang.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if server is ready\n",
    "import requests\n",
    "\n",
    "try:\n",
    "    r = requests.get('http://localhost:30000/health', timeout=5)\n",
    "    print(f\"Server ready: {r.json()}\")\n",
    "except:\n",
    "    print(\"Server not ready yet. Check logs:\")\n",
    "    !tail -20 /content/sglang.log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Generate Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import base64\n",
    "from IPython.display import Video, display\n",
    "\n",
    "BASE_URL = \"http://localhost:30000\"\n",
    "\n",
    "# Generate text-to-video\n",
    "response = requests.post(\n",
    "    f\"{BASE_URL}/v1/video/generations\",\n",
    "    json={\n",
    "        \"prompt\": \"A cat walking through a beautiful garden with colorful flowers\",\n",
    "        \"size\": \"832x480\",\n",
    "        \"num_frames\": 81,\n",
    "        \"num_inference_steps\": 50,\n",
    "        \"guidance_scale\": 5.0,\n",
    "        \"response_format\": \"b64_json\"\n",
    "    },\n",
    "    timeout=1200\n",
    ")\n",
    "\n",
    "if response.status_code == 200:\n",
    "    data = response.json()\n",
    "    video_bytes = base64.b64decode(data[\"data\"][0][\"b64_json\"])\n",
    "    \n",
    "    with open(\"/content/output.mp4\", \"wb\") as f:\n",
    "        f.write(video_bytes)\n",
    "    \n",
    "    print(\"Video generated successfully!\")\n",
    "    display(Video(\"/content/output.mp4\", embed=True))\n",
    "else:\n",
    "    print(f\"Error: {response.status_code}\")\n",
    "    print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. More Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate another video with different prompt\n",
    "prompts = [\n",
    "    \"A rocket launching into space with flames and smoke\",\n",
    "    \"Ocean waves crashing on a sandy beach at sunset\",\n",
    "    \"A person walking through a snowy forest\"\n",
    "]\n",
    "\n",
    "for i, prompt in enumerate(prompts):\n",
    "    print(f\"Generating video {i+1}: {prompt[:50]}...\")\n",
    "    \n",
    "    response = requests.post(\n",
    "        f\"{BASE_URL}/v1/video/generations\",\n",
    "        json={\n",
    "            \"prompt\": prompt,\n",
    "            \"size\": \"832x480\",\n",
    "            \"num_frames\": 81,\n",
    "            \"num_inference_steps\": 50,\n",
    "            \"response_format\": \"b64_json\"\n",
    "        },\n",
    "        timeout=1200\n",
    "    )\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        video_bytes = base64.b64decode(data[\"data\"][0][\"b64_json\"])\n",
    "        \n",
    "        with open(f\"/content/video_{i+1}.mp4\", \"wb\") as f:\n",
    "            f.write(video_bytes)\n",
    "        \n",
    "        print(f\"  Saved to video_{i+1}.mp4\")\n",
    "        display(Video(f\"/content/video_{i+1}.mp4\", embed=True))\n",
    "    else:\n",
    "        print(f\"  Error: {response.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Download Generated Videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "# Download videos to your local machine\n",
    "files.download('/content/output.mp4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Troubleshooting\n",
    "\n",
    "**Server won't start:**\n",
    "```python\n",
    "!cat /content/sglang.log\n",
    "```\n",
    "\n",
    "**Out of memory:**\n",
    "- Use smaller model: `Wan2.2-TI2V-5B-Diffusers`\n",
    "- Reduce `num_frames` to 41\n",
    "- Use smaller `size`: \"480x272\"\n",
    "\n",
    "**udocker GPU issues:**\n",
    "```python\n",
    "!udocker setup --nvidia --force wan22\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}